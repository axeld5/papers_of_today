{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CS_CLASSES = [\n",
    "    'cs.' + cat for cat in [\n",
    "        'AI', 'AR', 'CC', 'CE', 'CG', 'CL', 'CR', 'CV', 'CY', 'DB',\n",
    "        'DC', 'DL', 'DM', 'DS', 'ET', 'FL', 'GL', 'GR', 'GT', 'HC',\n",
    "        'IR', 'IT', 'LG', 'LO', 'MA', 'MM', 'MS', 'NA', 'NE', 'NI',\n",
    "        'OH', 'OS', 'PF', 'PL', 'RO', 'SC', 'SD', 'SE', 'SI', 'SY',\n",
    "    ]\n",
    "]\n",
    "DATE = \"2024-06-03\" #Must be of format YYYY-MM-DD\n",
    "TOPIC = \"Natural Language Processing, Generative AI, Computer Vision\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering papers through ArXiV API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arxiv api\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def get_arxiv_papers(date):\n",
    "    papers = []    \n",
    "    title_list = []\n",
    "    for cls in CS_CLASSES:\n",
    "        query = f\"http://export.arxiv.org/api/query?search_query=cat:'{cls}'&sortBy=submittedDate&sortOrder=descending\"\n",
    "        params = {\n",
    "                \"start\": 0,\n",
    "                \"max_results\": 300\n",
    "        }\n",
    "        response = requests.get(query, params=params)\n",
    "        if response.status_code == 200:\n",
    "            root = ET.fromstring(response.content)\n",
    "            entries = root.findall('{http://www.w3.org/2005/Atom}entry')\n",
    "            for entry in entries:\n",
    "                paper = {\n",
    "                    'id': entry.find('{http://www.w3.org/2005/Atom}id').text,\n",
    "                    'title': entry.find('{http://www.w3.org/2005/Atom}title').text,\n",
    "                    'summary': entry.find('{http://www.w3.org/2005/Atom}summary').text,\n",
    "                    'authors': [author.find('{http://www.w3.org/2005/Atom}name').text for author in entry.findall('{http://www.w3.org/2005/Atom}author')],\n",
    "                    'published': entry.find('{http://www.w3.org/2005/Atom}published').text\n",
    "                }\n",
    "                paper[\"author_count\"] = len(paper[\"authors\"])\n",
    "                if date in paper[\"published\"]:\n",
    "                    if paper[\"title\"] not in title_list:\n",
    "                        papers.append(paper)\n",
    "                        title_list.append(paper[\"title\"])\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "papers = get_arxiv_papers(DATE)\n",
    "print(len(papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "prompt = \"\"\"You are being given the abstract of a paper. Is this paper about {topic}, or not?\n",
    "Answer \"YES\" if the abstract is related to {topic}, and \"NO\" otherwise.\n",
    "<abstract> {abstract} </abstract>\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Recent work has provided indirect evidence that pretraining language models\n",
      "on code improves the ability of models to track state changes of discourse\n",
      "entities expressed in natural language. In this work, we systematically test\n",
      "this claim by comparing pairs of language models on their entity tracking\n",
      "performance. Critically, the pairs consist of base models and models trained on\n",
      "top of these base models with additional code data. We extend this analysis to\n",
      "additionally examine the effect of math training, another highly structured\n",
      "data type, and alignment tuning, an important step for enhancing the usability\n",
      "of models. We find clear evidence that models additionally trained on large\n",
      "amounts of code outperform the base models. On the other hand, we find no\n",
      "consistent benefit of additional math training or alignment tuning across\n",
      "various model families.\n",
      "\n",
      "YES\n",
      "  Recurrent neural networks (RNNs) notoriously struggle to learn long-term\n",
      "memories, primarily due to vanishing and exploding gradients. The recent\n",
      "success of state-space models (SSMs), a subclass of RNNs, to overcome such\n",
      "difficulties challenges our theoretical understanding. In this paper, we delve\n",
      "into the optimization challenges of RNNs and discover that, as the memory of a\n",
      "network increases, changes in its parameters result in increasingly large\n",
      "output variations, making gradient-based learning highly sensitive, even\n",
      "without exploding gradients. Our analysis further reveals the importance of the\n",
      "element-wise recurrence design pattern combined with careful parametrizations\n",
      "in mitigating this effect. This feature is present in SSMs, as well as in other\n",
      "architectures, such as LSTMs. Overall, our insights provide a new explanation\n",
      "for some of the difficulties in gradient-based learning of RNNs and why some\n",
      "architectures perform better than others.\n",
      "\n",
      "YES\n",
      "  Branch-and-bound (BaB) is among the most effective methods for neural network\n",
      "(NN) verification. However, existing works on BaB have mostly focused on NNs\n",
      "with piecewise linear activations, especially ReLU networks. In this paper, we\n",
      "develop a general framework, named GenBaB, to conduct BaB for general\n",
      "nonlinearities in general computational graphs based on linear bound\n",
      "propagation. To decide which neuron to branch, we design a new branching\n",
      "heuristic which leverages linear bounds as shortcuts to efficiently estimate\n",
      "the potential improvement after branching. To decide nontrivial branching\n",
      "points for general nonlinear functions, we propose to optimize branching points\n",
      "offline, which can be efficiently leveraged during verification with a lookup\n",
      "table. We demonstrate the effectiveness of our GenBaB on verifying a wide range\n",
      "of NNs, including networks with activation functions such as Sigmoid, Tanh,\n",
      "Sine and GeLU, as well as networks involving multi-dimensional nonlinear\n",
      "operations such as multiplications in LSTMs and Vision Transformers. Our\n",
      "framework also allows the verification of general nonlinear computation graphs\n",
      "and enables verification applications beyond simple neural networks,\n",
      "particularly for AC Optimal Power Flow (ACOPF). GenBaB is part of the latest\n",
      "$\\alpha,\\!\\beta$-CROWN, the winner of the 4th International Verification of\n",
      "Neural Networks Competition (VNN-COMP 2023).\n",
      "\n",
      "NO\n",
      "  Organic weed control is a vital to improve crop yield with a sustainable\n",
      "approach. In this work, a directed energy weed control robot prototype\n",
      "specifically designed for organic farms is proposed. The robot uses a novel\n",
      "distributed array robot (DAR) unit for weed treatment. Soybean and corn\n",
      "databases are built to train deep learning neural nets to perform weed\n",
      "recognition. The initial deep learning neural nets show a high performance in\n",
      "classifying crops. The robot uses a patented directed energy plant eradication\n",
      "recipe that is completely organic and UV-C free, with no chemical damage or\n",
      "physical disturbance to the soil. The deep learning can classify 8 common weed\n",
      "species in a soybean field under natural environment with up to 98% accuracy.\n",
      "\n",
      "NO\n",
      "  Large Language Models (LLMs) struggle with reliably generating highly\n",
      "structured outputs, such as program code, mathematical formulas, or well-formed\n",
      "markup. Constrained decoding approaches mitigate this problem by greedily\n",
      "restricting what tokens an LLM can output at each step to guarantee that the\n",
      "output matches a given constraint. Specifically, in grammar-constrained\n",
      "decoding (GCD), the LLM's output must follow a given grammar. In this paper we\n",
      "demonstrate that GCD techniques (and in general constrained decoding\n",
      "techniques) can distort the LLM's distribution, leading to outputs that are\n",
      "grammatical but appear with likelihoods that are not proportional to the ones\n",
      "given by the LLM, and so ultimately are low-quality. We call the problem of\n",
      "aligning sampling with a grammar constraint, grammar-aligned decoding (GAD),\n",
      "and propose adaptive sampling with approximate expected futures (ASAp), a\n",
      "decoding algorithm that guarantees the output to be grammatical while provably\n",
      "producing outputs that match the conditional probability of the LLM's\n",
      "distribution conditioned on the given grammar constraint. Our algorithm uses\n",
      "prior sample outputs to soundly overapproximate the future grammaticality of\n",
      "different output prefixes. Our evaluation on code generation and structured NLP\n",
      "tasks shows how ASAp often produces outputs with higher likelihood (according\n",
      "to the LLM's distribution) than existing GCD techniques, while still enforcing\n",
      "the desired grammatical constraints.\n",
      "\n",
      "YES\n"
     ]
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are an helpful AI assistant\"), (\"user\", prompt)]\n",
    ")\n",
    "chain = prompt_template | model | parser\n",
    "abstracts_list = [paper[\"summary\"] for paper in papers[:5]]\n",
    "for abstract in abstracts_list:\n",
    "    print(abstract)\n",
    "    print(chain.invoke({\"abstract\":abstract, \"topic\":TOPIC}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "selected_papers = []\n",
    "for i, paper in enumerate(papers):\n",
    "    abstract = paper[\"summary\"]\n",
    "    if i%100 == 0:\n",
    "        time.sleep(20) #time.sleep is there to avoid hitting anthropic rate limits\n",
    "    if chain.invoke({\"abstract\":abstract, \"topic\":TOPIC}) == \"YES\":\n",
    "        selected_papers.append(paper)\n",
    "print(len(selected_papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 papers extracted for date 2024-05-31 and topics: Natural Language Processing, Generative AI, Computer Vision\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(selected_papers)\n",
    "df = df.drop_duplicates(subset=\"title\") #duplicates can sometimes be found\n",
    "print(f\"{len(df)} papers extracted for date {DATE} and topics: {TOPIC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"extracted_papers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "info_dict = {\"date\": DATE, \"topic\": TOPIC}\n",
    "with open(\"parameters.json\", \"w\") as outfile: \n",
    "    json.dump(info_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
