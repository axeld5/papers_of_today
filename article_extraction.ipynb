{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CS_CLASSES = [\n",
    "    'cs.' + cat for cat in [\n",
    "        'AI', 'AR', 'CC', 'CE', 'CG', 'CL', 'CR', 'CV', 'CY', 'DB',\n",
    "        'DC', 'DL', 'DM', 'DS', 'ET', 'FL', 'GL', 'GR', 'GT', 'HC',\n",
    "        'IR', 'IT', 'LG', 'LO', 'MA', 'MM', 'MS', 'NA', 'NE', 'NI',\n",
    "        'OH', 'OS', 'PF', 'PL', 'RO', 'SC', 'SD', 'SE', 'SI', 'SY',\n",
    "    ]\n",
    "]\n",
    "DATE = \"2024-05-21\" #Must be of format YYYY-MM-DD\n",
    "TOPIC = \"Natural Language Processing, Generative AI, Computer Vision\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering papers through ArXiV API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arxiv api\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def get_arxiv_papers(date):\n",
    "    papers = []    \n",
    "    for cls in CS_CLASSES:\n",
    "        query = f\"http://export.arxiv.org/api/query?search_query=cat:'{cls}'&sortBy=submittedDate&sortOrder=descending\"\n",
    "        params = {\n",
    "                \"start\": 0,\n",
    "                \"max_results\": 1000\n",
    "        }\n",
    "        response = requests.get(query, params=params)\n",
    "        if response.status_code == 200:\n",
    "            root = ET.fromstring(response.content)\n",
    "            entries = root.findall('{http://www.w3.org/2005/Atom}entry')\n",
    "            for entry in entries:\n",
    "                paper = {\n",
    "                    'id': entry.find('{http://www.w3.org/2005/Atom}id').text,\n",
    "                    'title': entry.find('{http://www.w3.org/2005/Atom}title').text,\n",
    "                    'summary': entry.find('{http://www.w3.org/2005/Atom}summary').text,\n",
    "                    'authors': [author.find('{http://www.w3.org/2005/Atom}name').text for author in entry.findall('{http://www.w3.org/2005/Atom}author')],\n",
    "                    'published': entry.find('{http://www.w3.org/2005/Atom}published').text\n",
    "                }\n",
    "                if date in paper[\"published\"]:\n",
    "                    papers.append(paper)\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n"
     ]
    }
   ],
   "source": [
    "papers = get_arxiv_papers(DATE)\n",
    "print(len(papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "prompt = \"\"\"You are being given the abstract of a paper. Is this paper about {topic}, or not?\n",
    "Answer \"YES\" if the abstract is related to {topic}, and \"NO\" otherwise.\n",
    "<abstract> {abstract} </abstract>\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching through chemical space is an exceptionally challenging problem\n",
      "because the number of possible molecules grows combinatorially with the number\n",
      "of atoms. Large, autoregressive models trained on databases of chemical\n",
      "compounds have yielded powerful generators, but we still lack robust strategies\n",
      "for generating molecules with desired properties. This molecular search problem\n",
      "closely resembles the \"alignment\" problem for large language models, though for\n",
      "many chemical tasks we have a specific and easily evaluable reward function.\n",
      "Here, we introduce an algorithm called energy rank alignment (ERA) that\n",
      "leverages an explicit reward function to produce a gradient-based objective\n",
      "that we use to optimize autoregressive policies. We show theoretically that\n",
      "this algorithm is closely related to proximal policy optimization (PPO) and\n",
      "direct preference optimization (DPO), but has a minimizer that converges to an\n",
      "ideal Gibbs-Boltzmann distribution with the reward playing the role of an\n",
      "energy function. Furthermore, this algorithm is highly scalable, does not\n",
      "require reinforcement learning, and performs well relative to DPO when the\n",
      "number of preference observations per pairing is small. We deploy this approach\n",
      "to align molecular transformers to generate molecules with externally specified\n",
      "properties and find that it does so robustly, searching through diverse parts\n",
      "of chemical space. While our focus here is on chemical search, we also obtain\n",
      "excellent results on an AI supervised task for LLM alignment, showing that the\n",
      "method is scalable and general.\n",
      "\n",
      "YES\n",
      "  This paper addresses the challenge of enhancing cybersecurity in\n",
      "Blockchain-based Internet of Things (BIoTs) systems, which are increasingly\n",
      "vulnerable to sophisticated cyberattacks. It introduces an AI-powered system\n",
      "model for the dynamic deployment of honeypots, utilizing an Intrusion Detection\n",
      "System (IDS) integrated with smart contract functionalities on IoT nodes. This\n",
      "model enables the transformation of regular nodes into decoys in response to\n",
      "suspicious activities, thereby strengthening the security of BIoT networks. The\n",
      "paper analyses strategic interactions between potential attackers and the\n",
      "AI-enhanced IDS through a game-theoretic model, specifically Bayesian games.\n",
      "The model focuses on understanding and predicting sophisticated attacks that\n",
      "may initially appear normal, emphasizing strategic decision-making, optimized\n",
      "honeypot deployment, and adaptive strategies in response to evolving attack\n",
      "patterns.\n",
      "\n",
      "NO\n",
      "  Large Language Models (LLMs) have shown remarkable capabilities in tasks such\n",
      "as summarization, arithmetic reasoning, and question answering. However, they\n",
      "encounter significant challenges in the domain of moral reasoning and ethical\n",
      "decision-making, especially in complex scenarios with multiple stakeholders.\n",
      "This paper introduces the Skin-in-the-Game (SKIG) framework, aimed at enhancing\n",
      "moral reasoning in LLMs by exploring decisions' consequences from multiple\n",
      "stakeholder perspectives. Central to SKIG's mechanism is simulating\n",
      "accountability for actions, which, alongside empathy exercises and risk\n",
      "assessment, is pivotal to its effectiveness. We validate SKIG's performance\n",
      "across various moral reasoning benchmarks with proprietary and opensource LLMs,\n",
      "and investigate its crucial components through extensive ablation analyses.\n",
      "\n",
      "YES\n",
      "  The emergence of generative artificial intelligence (GenAI) is transforming\n",
      "information interaction. For decades, search engines such as Google and Bing\n",
      "have been the primary means of locating relevant information for the general\n",
      "population. They have provided search results in the same standard format (the\n",
      "so-called \"10 blue links\"). The recent ability to chat via natural language\n",
      "with AI-based agents and have GenAI automatically synthesize answers in\n",
      "real-time (grounded in top-ranked results) is changing how people interact with\n",
      "and consume information at massive scale. These two information interaction\n",
      "modalities (traditional search and AI-powered chat) coexist in current search\n",
      "engines, either loosely coupled (e.g., as separate options/tabs) or tightly\n",
      "coupled (e.g., integrated as a chat answer embedded directly within a\n",
      "traditional search result page). We believe that the existence of these two\n",
      "different modalities, and potentially many others, is creating an opportunity\n",
      "to re-imagine the search experience, capitalize on the strengths of many\n",
      "modalities, and develop systems and strategies to support seamless flow between\n",
      "them. We refer to these as panmodal experiences. Unlike monomodal experiences,\n",
      "where only one modality is available and/or used for the task at hand, panmodal\n",
      "experiences make multiple modalities available to users (multimodal), directly\n",
      "support transitions between modalities (crossmodal), and seamlessly combine\n",
      "modalities to tailor task assistance (transmodal). While our focus is search\n",
      "and chat, with learnings from insights from a survey of over 100 individuals\n",
      "who have recently performed common tasks on these two modalities, we also\n",
      "present a more general vision for the future of information interaction using\n",
      "multiple modalities and the emergent capabilities of GenAI.\n",
      "\n",
      "YES\n",
      "  This paper addresses a critical gap in legal analytics by developing and\n",
      "applying a novel taxonomy for topic modelling summary judgment cases in the\n",
      "United Kingdom. Using a curated dataset of summary judgment cases, we use the\n",
      "Large Language Model Claude 3 Opus to explore functional topics and trends. We\n",
      "find that Claude 3 Opus correctly classified the topic with an accuracy of\n",
      "87.10%. The analysis reveals distinct patterns in the application of summary\n",
      "judgments across various legal domains. As case law in the United Kingdom is\n",
      "not originally labelled with keywords or a topic filtering option, the findings\n",
      "not only refine our understanding of the thematic underpinnings of summary\n",
      "judgments but also illustrate the potential of combining traditional and\n",
      "AI-driven approaches in legal classification. Therefore, this paper provides a\n",
      "new and general taxonomy for UK law. The implications of this work serve as a\n",
      "foundation for further research and policy discussions in the field of judicial\n",
      "administration and computational legal research methodologies.\n",
      "\n",
      "YES\n"
     ]
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are an helpful AI assistant\"), (\"user\", prompt)]\n",
    ")\n",
    "chain = prompt_template | model | parser\n",
    "abstracts_list = [paper[\"summary\"] for paper in papers[:5]]\n",
    "for abstract in abstracts_list:\n",
    "    print(abstract)\n",
    "    print(chain.invoke({\"abstract\":abstract, \"topic\":TOPIC}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "selected_papers = []\n",
    "for i, paper in enumerate(papers):\n",
    "    abstract = paper[\"summary\"]\n",
    "    if i%30 == 0:\n",
    "        time.sleep(20) #time.sleep is there to avoid hitting anthropic rate limits\n",
    "    if chain.invoke({\"abstract\":abstract, \"topic\":TOPIC}) == \"YES\":\n",
    "        print(paper[\"title\"])\n",
    "        selected_papers.append(paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 papers extracted for date 2024-05-21 and topics: Natural Language Processing, Generative AI, Computer Vision\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(selected_papers)\n",
    "df = df.drop_duplicates(subset=\"title\") #duplicates can sometimes be found\n",
    "print(f\"{len(df)} papers extracted for date {DATE} and topics: {TOPIC}\")\n",
    "df.to_csv(f\"extracted_papers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "info_dict = {\"date\": DATE, \"topic\": TOPIC}\n",
    "with open(\"parameters.json\", \"w\") as outfile: \n",
    "    json.dump(info_dict, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
